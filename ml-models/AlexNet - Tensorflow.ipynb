{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of AlexNet architecture[1]\n",
    "#### Created by 'Nishchal Gaba' (nishgaba9@gmail.com)(October 2017)\n",
    "#### GitHub : https://github.com/nishgaba-ai/Machine-Learning/ml-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "#### [1] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).\n",
    "\n",
    "#### NOTE: The code is not broken down into multiple GPU implementation as in the original paper [1]\n",
    "#### NOTE: At some places, different functions are also provided to be able to test different parameters and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import files\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy import misc\n",
    "import glob\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "plt.pyplot.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for convolution, pooling, droput, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1> Fully Connected Layer with RELU Glorot Initialization and Exponential Linear Units (ELUs)\n",
    "#### Glorot Initialization (Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249-256).)\n",
    "#### Exponential Linear Units (ELUs) (Clevert, D. A., Unterthiner, T., & Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RELU GLOROT INITIALIZATION\n",
    "def fully_connected_layer_relu(inputs, input_dim, output_dim, nonlinearity=tf.nn.relu):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [input_dim, output_dim], stddev=2. / (input_dim + output_dim)**0.5), \n",
    "        'weights')\n",
    "    biases = tf.Variable(tf.zeros([output_dim]), 'biases')\n",
    "    outputs = nonlinearity(tf.matmul(inputs, weights) + biases)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Exponential Linear Units (ELUs) for activation to test the performace difference\n",
    "def fully_connected_layer_elu(inputs, input_dim, output_dim, nonlinearity=tf.nn.elu):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [input_dim, output_dim], stddev=2. / (input_dim + output_dim)**0.5), \n",
    "        'weights')\n",
    "    biases = tf.Variable(tf.zeros([output_dim]), 'biases')\n",
    "    outputs = nonlinearity(tf.matmul(inputs, weights) + biases)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2> Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout Layer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/dropout\n",
    "# The default dropout rate is kept at '0.5'\n",
    "def dropout(input, drop_rate = 0.5):\n",
    "    # Uses tensorflow's dropout method\n",
    "    return tf.nn.droput(input, drop_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3> Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ksize = Kernel Size\n",
    "# strides = Strides for Pooling Operation\n",
    "# padding = Padding Style for Convolution\n",
    "# NOTE: You can modify these according to your project requirements\n",
    "\n",
    "# Max Pooling Layer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\n",
    "def max_pool(inputs, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='VALID'):\n",
    "   return tf.nn.max_pool(inputs, ksize, strides, padding, name='max_pool')\n",
    "\n",
    "# Average Pooling Layer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool\n",
    "def avg_pool(inputs, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],padding='VALID'):\n",
    "   return tf.nn.avg_pool(conv1, ksize, strides,padding, name='avg_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4> Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out : https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/normalization\n",
    "# For the documentation of the input arguments for the normalization\n",
    "# The depth radius in some implementations is 2 for other AlexNet architectures\n",
    "def local_response_normalization(inputs,depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75):\n",
    "    return tf.nn.lrn(inputs,depth_radius, bias, alpha, beta, name='Local Response Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5> Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "def conv(inputs,weights, strides = [1,1,1,1], padding = 'SAME'):\n",
    "    return tf.nn.conv2d(inputs, weights, strides, padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6> Initial Weights, Bias and RELU Activation for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The initial weights and biases are provided with a shape and size\n",
    "# Hence, they can be modified according to the requirements of your project, this is truncated normal distribution\n",
    "# For details on truncated normal : https://www.tensorflow.org/api_docs/python/tf/truncated_normal\n",
    "\n",
    "# Initializing Weights with truncated normal distribution\n",
    "def initial_weights_for_layers(shape, stdev=5e-2):\n",
    "    return tf.Variable(tf.truncated_normal('initial_weights',shape, stddev=stdev))\n",
    "\n",
    "# Initializing Biases\n",
    "def initial_biases(size, value):\n",
    "    return tf.Variable(tf.constant(value, shape=[size]), 'initial_biases')\n",
    "\n",
    "# RELU ACTIVATION\n",
    "def relu_activation(input):\n",
    "    input = tf.nn.relu(input, name = 'RELU')\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your images as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We assume the images to be stored in a numpy's array 'inputs'\n",
    "# NOTE: Here we have not loaded any images currently as inputs\n",
    "# For implementation of this architecture for testing on different datasets such as MNIST, SVHN, etc. \n",
    "# Check out my github's 'Machine Learning' repository for the updates and implementation \n",
    "\n",
    "# The paper takes the input size to be = 224 x 224 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We name the input images as 'inputs' and outputs as 'outputs'\n",
    "# Although for this notebook, they are just like empty placeholders as this is the description of the structure\n",
    "\n",
    "# For adding the biases, we use: https://www.tensorflow.org/api_docs/python/tf/nn/bias_add\n",
    "\n",
    "# First Convolutional Layer\n",
    "with tf.name_scope('Convolution - 1'):\n",
    "    # Initializing weights and biases for this layer\n",
    "    weights1 = initial_weights_for_layers(shape=[11, 11, 3, 96])\n",
    "    biases1 = initial_biases(size = 96, value = 0.0)\n",
    "    \n",
    "    \n",
    "    # Convolution-1\n",
    "    # First layer filters the input with '96' kernels of size '11 x 11 x 3'\n",
    "    # conv_o is just like a dummy to hold the initial convolution\n",
    "    conv_o= conv(inputs, weights1, [1, 4, 4, 1], padding='SAME')\n",
    "    # Adding the biases\n",
    "    conv_o = tf.nn.bias_add(conv_o, biases1)\n",
    "    \n",
    "    # ReLU Activation\n",
    "    conv1 = relu_activation(conv_o)\n",
    "    \n",
    "    # Normalized, Takes the conv1 as input\n",
    "    norm1 = local_response_normalization(conv1)\n",
    "    \n",
    "    # Pooling, Takes norm1 as input\n",
    "    pool1 = max_pool(norm1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# The initial biases here have been changed to '0.1' rather than '0.0' to test some difference,\n",
    "# Although in other implementations you will find this initial value to '0.0' which can be changed accordingly    \n",
    "# Second Convolutional Layer\n",
    "with tf.name_scope('Convolution - 2'):\n",
    "    # Initializing weights and biases for this layer\n",
    "    weights2 = initial_weights_for_layers(shape=[5, 5, 48, 256])\n",
    "    biases1 = initial_biases(size = 256, value = 0.1)\n",
    "    \n",
    "    # Convolution-2\n",
    "    # Second layer filters the input with '256' kernels of size '5 x 5 x 48'\n",
    "    # conv_o is just like a dummy to hold the initial convolution\n",
    "    conv_o = conv(pool1, weights2, [1, 1, 1, 1], padding='SAME')\n",
    "    # Adding the biases\n",
    "    conv_o = tf.nn.bias_add(conv_o, biases2)\n",
    "    \n",
    "    # ReLU Activation\n",
    "    conv2 = relu_activation(conv_o)\n",
    "    \n",
    "    # Normalized, Takes the conv2 as input\n",
    "    norm2 = local_response_normalization(conv2)\n",
    "    \n",
    "    # Pooling, Takes norm2 as input\n",
    "    pool2 = max_pool(norm2)\n",
    "\n",
    "\n",
    "###  \n",
    "# NOTE: There is no normalization and pooling in 3rd, 4th and 5th convolution layer\n",
    "###\n",
    "# Third Convolutional Layer\n",
    "with tf.name_scope('Convolution - 3'):\n",
    "    # Initializing weights and biases for this layer\n",
    "    weights3 = initial_weights_for_layers(shape=[3, 3, 256, 384])\n",
    "    biases3 = initial_biases(size = 384, value = 0.0)\n",
    "    \n",
    "    \n",
    "    # Convolution-3\n",
    "    # Third layer filters the input with '384' kernels of size '3 x 3 x 256'\n",
    "    # conv_o is just like a dummy to hold the initial convolution\n",
    "    conv_o = conv(pool2, weights3, [1, 1, 1, 1], padding='SAME')\n",
    "    # Adding the biases\n",
    "    conv_o = tf.nn.bias_add(conv_o, biases3)\n",
    "    \n",
    "    # ReLU Activation\n",
    "    conv3 = relu_activation(conv_o)\n",
    "    \n",
    "# Fourth Convolutional Layer\n",
    "with tf.name_scope('Convolution - 4'):\n",
    "    # Initializing weights and biases for this layer\n",
    "    weights4 = initial_weights_for_layers(shape=[3, 3, 192, 384])\n",
    "    biases4 = initial_biases(size = 384, value = 0.0)\n",
    "    \n",
    "    # Convolution-4\n",
    "    # Fourth layer filters the input with '384' kernels of size '3 x 3 x 192'\n",
    "    # conv_o is just like a dummy to hold the initial convolution\n",
    "    conv_o = conv(conv3, weights4, [1, 1, 1, 1], padding='SAME')\n",
    "    # Adding the biases\n",
    "    conv_o = tf.nn.bias_add(conv_o, biases4)\n",
    "    \n",
    "    # ReLU Activation\n",
    "    conv4 = relu_activation(conv_o)\n",
    "    \n",
    "# Fifth Convolutional Layer\n",
    "with tf.name_scope('Convolution - 5'):\n",
    "    # Initializing weights and biases for this layer\n",
    "    weights5 = initial_weights_for_layers(shape=[3, 3, 192, 256])\n",
    "    biases5 = initial_biases(size = 256, value = 0.0)\n",
    "    \n",
    "    # Convolution-5\n",
    "    # Fifth layer filters the input with '256' kernels of size '3 x 3 x 192'\n",
    "    # conv_o is just like a dummy to hold the initial convolution\n",
    "    conv_o = conv(conv4, weights5, [1, 1, 1, 1], padding='SAME')\n",
    "    # Adding the biases\n",
    "    conv_o = tf.nn.bias_add(conv_o, biases5)\n",
    "    \n",
    "    # ReLU Activation\n",
    "    conv5 = relu_activation(conv_o)\n",
    "    \n",
    "    \n",
    "# Reshaping the inputs to be passed to the fully connected layers, as the when passing the output of previous layers\n",
    "# We want to separate out the dimensions as (batch_size, dimensions), where the batch_size can be dynamically calculated\n",
    "# REFER TO : https://www.tensorflow.org/tutorials/layers\n",
    "# To understand the formation of a neural network and hence the requirement of this operation\n",
    "\n",
    "# NOTE: HERE INSTEAD OF THE ORIGINAL DIMENSIONS OF THE ALEXNET\n",
    "# I am including for ease the variables which you can change to suit the requirements of your project\n",
    "# This would help you to understand how the dimensions vary according to different datasets\n",
    "a = 8 # this is just an example\n",
    "b = 8 # this is just an example\n",
    "c = 64 # this is just an example\n",
    "# This  can be understood as the dimensions of the input image\n",
    "\n",
    "reshaped_inputs = tf.reshape(conv5, [-1,a*b*c]) \n",
    "    \n",
    "# NOTE: The a,b,c parameters may vary from original implementation\n",
    "###\n",
    "# NOTE: Fully Connected layers have '4096' units each\n",
    "# NOTE: Dropout is used in the first 2 fully connected layers\n",
    "###\n",
    "# First Fully Connected Layer\n",
    "with tf.name_scope('Fully Connected - 1'):\n",
    "    fc=fully_connected_layer_relu(reshaped_input, a*b*c, 4096)\n",
    "    fc1 = dropout(fc)\n",
    "\n",
    "# Second Fully Connected Layer\n",
    "with tf.name_scope('Fully Connected - 2'):\n",
    "    fc=fully_connected_layer_relu(fc1, 4096, 4096)\n",
    "    fc2 = dropout(fc)\n",
    "\n",
    "# Third Fully Connected Layer\n",
    "with tf.name_scope('Fully Connected - 3'):\n",
    "    fc3=fully_connected_layer_relu(fc2, 4096, 4096)\n",
    "\n",
    "# Output of third fully connected layer is passed to a 1000-way softmax classifier\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "with tf.name_scope('Softmax'): \n",
    "    fc4=tf.nn.softmax(fc3,1000,name='softmax')\n",
    "\n",
    "with tf.name_scope('Outputs'):\n",
    "    outputs =fc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:The tf.session() is not provided here as this is a structure of the AlexNet Model,\n",
    "### As mentioned earlier, for implementation of this model on diferent datasets, you can check the repository mentioned at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
